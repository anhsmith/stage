---
title: "The STAGE Likelihood"
output: rmarkdown::html_vignette
bibliography: ../inst/references/ref.bib
vignette: >
  %\VignetteIndexEntry{The STAGE Likelihood}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>"
)

library(stage)
library(tidyverse)
```

# Overview

This vignette describes the **likelihood and parameterisation** used in the STAGE model.

STAGE is a **hierarchical Bayesian generative model** designed to estimate a
**transition point** \(m_{50}\) on variable \(x\) between two classes—for example, the length at which individuals
transition from immature to mature. The model produces a posterior
distribution for key parameter \(m_{50}\), representing the value of \(x\) at which the probabilities \(Pr(y=0)\) and \(\Pr(y=1)\) are equal. 
The model can also be used to derive other useful quantities (such as class probabilities
\(\Pr(y = 1 \mid x)\)).

The central idea is to model each class with a **piecewise density** that combines a **uniform plateau** in regions far from the transition, and a **Gaussian tail** in the transition region. The left group (\(y = 0\)) is modelled with a uniform on the left that begins at the lower truncation value \(L\). The distribution remains flat until it approaches the transition range, at which point the distribution declines according to a Gaussian. The right group (\(y = 1\)) is a mirror image of the left—it increases according to a Gaussian distribution and then levels off to a uniform, terminating at the upper truncation value \(U\). 

The model focuses inference on the **transition zone**, where the values of \(x\) of the two classes overlap, thereby limiting the influence of
observations that are biologically uninformative because they are far away from the transition point.

The PDF of the uniform-Gaussian used here was modified from the PDF uniform and two Guassians described by Lau & Krumscheid [-@lau_plateau_2022].

# Model summary

We observe pairs \((x_i, y_i)\) for \(i = 1, \dots, N\), where

- \(x_i\) is a continuous covariate (e.g., length), truncated to an interval
  \([L, U]\), and  
- \(y_i \in \{0, 1\}\) is a binary indicator of state (e.g., immature / mature).

The model assumes that the distribution of \(x\) conditional on state is **piecewise**:

- For \(y = 0\) (immature individuals), the distribution of \(x\) is approximately
  **uniform** up to a point \(\mu_0\), after which it follows a **Gaussian decay**.
- For \(y = 1\) (mature individuals), the distribution is **Gaussian** below a point
  \(\mu_1\), and **uniform** above \(\mu_1\) up to the upper truncation \(U\).

The **transition point** \(m_{50}\) is defined as the value of \(x\) where the
probability of observing either class is equal:

\[
\Pr(y = 0 \mid x = m_{50}) = \Pr(y = 1 \mid x = m_{50}).
\]

Additional parameters govern the **width** and **sharpness** of the transition:

- \(d\) controls the distance between \(\mu_0\) and \(\mu_1\) (the width of the
  transition region),
- \(\sigma\) controls the spread (standard deviation) of the Gaussian tails.

By allowing different behaviours on either side of \(m_{50}\), the model can capture
**asymmetric transition patterns**: the immature class can decline towards the
transition at a different rate than the mature class rises out of it.

This is particularly useful in settings where:

- the boundary between classes is **not sharp**, and  
- the data exhibit **gradual overlap** between the two classes.


# Likelihood

The likelihood is built from two **unnormalised log-density functions** for the two
classes. We work on the log scale for numerical stability and then subtract
log-normalising constants to ensure valid densities.

Let \(f_0(x)\) and \(f_1(x)\) denote the unnormalised densities for the two classes.

## Lower class (\(y = 0\))

For \(y_i = 0\), the unnormalised piecewise log-density is:

\[
f(x_i \mid y_i = 0, \mu_0, \sigma, L) =
\begin{cases}
0 & \text{if } x_i < L \\
1 & \text{if } L \leq x_i \leq \mu_0 \\
-\dfrac{(x_i - \mu_0)^2}{2\sigma^2} & \text{if } x_i > \mu_0
\end{cases}
\]

Here:

- the plateau region \([L, \mu_0]\) has constant (log-)density 1,  
- the tail region \(x_i > \mu_0\) decays according to a Gaussian kernel.

In implementation, this is treated as an **unnormalised log-density**; the overall
density is recovered after subtracting a normalising constant.


## Higher class (\(y = 1\))

For \(y_i = 1\), the unnormalised piecewise log-density is:

\[
f(x_i \mid y_i = 1, \mu_1, \sigma, U) =
\begin{cases}
-\dfrac{(x_i - \mu_1)^2}{2\sigma^2} & \text{if } x_i < \mu_1 \\
1 & \text{if } \mu_1 \leq x_i \leq U \\
0 & \text{if } x_i > U
\end{cases}
\]

Now:

- the Gaussian tail is **below** \(\mu_1\), and  
- the plateau covers the upper region \([\mu_1, U]\).

In both cases, we are effectively using a **plateau–Gaussian** mixture: a flat
density where the class “dominates” and a Gaussian shape in the transition
region where the other class is present.


## Normalisation constants

To convert the unnormalised densities into proper probability density functions
(pdf), we divide by constants \(C_0\) and \(C_1\) chosen such that the densities
integrate to 1 over \([L, U]\).

For the immature class, the normalising constant is:

\[
C_0 = \frac{\sqrt{2\pi} \, \sigma}{2} + (\mu_0 - L).
\]

For the mature class, the normalising constant is:

\[
C_1 = \frac{\sqrt{2\pi} \, \sigma}{2} + (U - \mu_1).
\]

Intuitively:

- \((\mu_0 - L)\) and \((U - \mu_1)\) are the **widths of the uniform plateaus**,  
- \(\sqrt{2\pi}\,\sigma / 2\) is the **contribution from the half-Gaussian tail**.

The **log-likelihood contributions** for each observation are then:

\[
\log \mathcal{L}_i \;=\;
\begin{cases}
\log f(x_i) - \log C_0 & \text{if } y_i = 0 \\
\log f(x_i) - \log C_1 & \text{if } y_i = 1.
\end{cases}
\]


## Full log-likelihood

Summing over all observations, the joint log-likelihood is:

\[
\log p(x, y \mid m_{50}, d, \sigma) \;=\;
\sum_{i=1}^{N}
\left[
  \log f(x_i \mid y_i, \mu_{y_i}, \sigma)
  - \log C_{y_i}
\right],
\]

where

- \(f(\cdot)\) is the appropriate unnormalised density for class \(y_i\), and  
- \(C_{y_i} \in \{C_0, C_1\}\) is the normalising constant for that class.


# Prior specification

The priors reflect prior beliefs about plausible transition points and transition
widths in typical length-at-maturity problems (but can be changed by the user).

- **Transition point** \(m_{50}\):

\[
m_{50} \sim \mathcal{N}(1300, 50),
\]

which centres the transition around 1300 (mm) with moderate uncertainty.

- **Transition width** \(d\):

\[
d \sim \mathcal{N}(100, 100),
\]

which allows the width of the overlap region to vary around 100 units with a wide
spread.

- **Gaussian spread** \(\sigma\):

\[
\sigma \sim \mathcal{N}(100, 50),
\]

representing prior beliefs about the scale of variability in the transition tails.

In practice, you should adapt these priors to your scale (e.g., lengths, ages)
and prior biological knowledge. The STAGE implementation allows you to pass in
hyperparameters via helper functions (e.g. `stage_priors()`).


# Transformed parameters

The Gaussian “centres” for the two classes are defined in terms of the transition
point \(m_{50}\) and the width parameter \(d\):

\[
\mu_0 = m_{50} - \frac{d}{2}, \qquad
\mu_1 = m_{50} + \frac{d}{2}.
\]

Thus:

- \(d\) controls the **distance** between the immature and mature Gaussian means,  
- \(m_{50}\) remains the **midpoint** between them.

This is convenient both conceptually and computationally: it allows the model to
focus directly on \(m_{50}\) and \(d\), which are often the primary scientific
targets, while \(\mu_0\) and \(\mu_1\) are derived quantities.

# From class densities to class probabilities (Bayes rule)

The likelihood above defines the **class-conditional densities**  
\(f_0(x) = p(x \mid y = 0)\) and \(f_1(x) = p(x \mid y = 1)\) on \([L, U]\).

To obtain the probability that an individual with a particular \(x\) value is 
in the higher class (\(y = 1\)), we use Bayes rule:

\[
\Pr(y = 1 \mid x)
= \frac{\pi_1 f_1(x)}{\pi_0 f_0(x) + \pi_1 f_1(x)},
\]

and similarly

\[
\Pr(y = 0 \mid x)
= \frac{\pi_0 f_0(x)}{\pi_0 f_0(x) + \pi_1 f_1(x)},
\]

where \(\pi_0\) and \(\pi_1\) are the prior class probabilities  
(e.g., prior probabilities of being immature or mature *before* seeing \(x\)).

In many applications, such as estimating length-at-maturity,
we are interested in a transition point that is 
**not driven by sample sizes or prevalence**, so it is natural to use
**equal class priors**, \(\pi_0 = \pi_1 = 1/2\). In that case, Bayes rule
simplifies to

\[
\Pr(y = 1 \mid x)
= \frac{f_1(x)}{f_0(x) + f_1(x)}, \qquad
\Pr(y = 0 \mid x)
= \frac{f_0(x)}{f_0(x) + f_1(x)}.
\]

These are the class probabilities that the STAGE model uses for prediction, and
they are what you see when you plot \(\Pr(y = 1 \mid x)\) as a function of \(x\).


### Connection to \(m_{50}\)

By definition, the transition point \(m_{50}\) is the value of \(x\) where the
two classes are equally probable:

\[
\Pr(y = 0 \mid x = m_{50}) = \Pr(y = 1 \mid x = m_{50}) = \frac{1}{2}.
\]

Using Bayes rule with equal class priors, this equality is equivalent to

\[
\frac{f_1(m_{50})}{f_0(m_{50}) + f_1(m_{50})}
=
\frac{f_0(m_{50})}{f_0(m_{50}) + f_1(m_{50})}
\quad\Longleftrightarrow\quad
f_0(m_{50}) = f_1(m_{50}).
\]

Thus, \(m_{50}\) can be viewed in two equivalent ways:

1. As a **model parameter** that sets the midpoint between the Gaussian centres
   \(\mu_0\) and \(\mu_1\) via  
   \[
   \mu_0 = m_{50} - \frac{d}{2}, \qquad
   \mu_1 = m_{50} + \frac{d}{2},
   \]
2. As the **Bayes classifier cut-point** where the posterior class probabilities
   for immature and mature are both 0.5.

In practice, the STAGE model samples \(m_{50}\), \(d\), and \(\sigma\) from their
posterior distribution. For any posterior draw, we can compute
\(\Pr(y = 1 \mid x)\) via Bayes rule and verify that the point where this curve
crosses 0.5 aligns with the sampled value of \(m_{50}\). This makes the
interpretation of \(m_{50}\) as a “50% maturity” transition point directly linked
to the underlying generative model and Bayes-optimal classifier.


# Advantages of the STAGE likelihood

1. **Piecewise densities**

   The plateau–Gaussian form naturally captures situations where individuals in one
   class (e.g., immatures) are approximately uniform over a wide region, but then
   decline gradually into the transition. Similarly for the mature class. This is
   more realistic than assuming a global Gaussian for each class.

2. **Focus on the transition region**

   Because only the Gaussian tails enter the overlap region, the estimation of
   \(m_{50}\) is driven by observations near the transition. Distant observations
   (e.g., very small immatures, very large matures) mainly contribute through the
   uniform plateau and therefore do not distort the transition point.

3. **Flexibility via \(d\)**

   The parameter \(d\) controls the width of the transition zone, allowing the
   model to capture anything from very sharp to very gradual transitions.

4. **Bayesian inference**

   A fully Bayesian treatment:

   - incorporates prior knowledge (e.g., plausible ranges for \(m_{50}\)),  
   - provides full posterior uncertainty for all parameters,  
   - extends naturally to **hierarchical models** with multiple populations.

5. **Proper normalisation**

   The explicit normalising constants \(C_0\) and \(C_1\) ensure that each
   class-conditional density is a valid pdf. This is crucial for stable
   inference in Stan and for interpreting posterior predictions as probabilities.


# Relationship to the `stage` implementation

In the `stage` package:

- The single-population model uses parameters
  `m50`, `d`, and `sigma_x` that correspond to \(m_{50}\), \(d\), and \(\sigma\)
  in this vignette.
- The multi-population (hierarchical) model introduces population-specific
  transition points \(m_{50, j}\) via a random-effects structure:
  \[
  m50_{\text{pop}[j]} = \mu_{m50} + z_j \, \sigma_\alpha,
  \]
  where \(\mu_{m50}\) is the overall mean transition point, \(z_j\) are
  standard-normal effects, and \(\sigma_\alpha\) is the population-level
  standard deviation.

The Stan code used in `fit_stage()` is a direct implementation of the likelihood
and parameterisation described above.

Future vignettes will provide a more detailed walk-through of the Stan code and
extensions such as alternative priors and model comparison.

# References
